{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":60893,"databundleVersionId":7000181,"sourceType":"competition"},{"sourceId":6724823,"sourceType":"datasetVersion","datasetId":3873965}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Task Overview \nTask: In this synthetic dataset based off of a real dataset funded by the Mayo Clinic, each dataset row/example represents both general and survival information about a patient that has liver cirrhosis, a condition involving prolonged liver damage. The goal is to train a machine learning model that can predict the patient's current survival status based on the data features. \n\nApproach v1: Our approach will be to train a Logistic Regression model using sklearn to be used as a baseilne model and to then train a performance-focused model using XGBoost as a learning exercise.\n\nApproach v2: After a few days of initial work, we realized that it was feasible within the competition timeframe to improve on Approach 1 by training an Ensemble of models so we changed our approach to training multiple Logistic Regression, Random Forest, and XGBoost models and then combining them into an ensemble.\n\nApproach v3: After manual hyperparameter tuning all the models, we realized that the current Logistic Regression and Random Forest models were too weak to be included in the final Ensemble model. Instead we decided to add a LGBM classifier and create an Ensemble model with a LGBM classifier + XGBoost classifier. We also implemented k-fold cross validation for hyperparameter tuning to reduce variance in the results compared to the validation set approach. We also added the original dataset to the synthetic dataset which improved our results significantly. \n\nVersion History\n1. v1.0-1.3 - Implemented dataset loading and dataset pre-processing. Added explanations and comments for each step. \n2. v1.4 - Added Logistic Regression model training and Kaggle submission formatting\n3. v2.0 - Added Random Forest and XGBoost model training and prediction.\n4. v2.1 - Added log-loss calculations for each model on the validation set. \n5. v2.2 - Added initial hyperparameter tuning for Logistic Regression, Random Forest, and XGBoost models. \n6. v3.0 - Added LGBM Classifier and did hyperparameter tuning for the LGBM Classifier. Added explanations for model training / tuning\n7. v3.1 - Combined the tuned LGBM Classifier and XGBoost classifer into an Ensemble model. \n8. v3.2 - Switched to K-Fold Cross Validation approach for all hyperparameter tuning. \n9. v3.3 - Combined original dataset with the synthetic dataset for training and updated log-loss scores accordingly. Did further hyperparameter tuning. \n\nLog-Loss Score Logs (Validation Scores using Synthetic Dataset only)\n1. Default/Tuned Logistic Regression - 0.525 / 0.525\n2. Default/Tuned Random Forest - 0.550 / 0.482\n3. Default/Tuned XGBoost - 0.515 / 0.437 \n4. Default/Tuned LGBM - 0.472 / 0.425\n5. Tuned XGBoost / LGBM Ensemble Model - 0.431\n\nLog-Loss Score Logs (Validation Scores using Both the Synthetic Dataset and Original Dataset)\n1. Tuned Logistic Regression - 0.53\n2. Tuned Random Forest - 0.507\n3. Tuned XGBoost - 0.414\n4. Tuned LGBM - 0.409\n5. Tuned XGBoost / LGBM Ensemble Model - 0.409","metadata":{}},{"cell_type":"markdown","source":"# Dataset Loading","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n# Read in the initial competition training and test data as Pandas DataFrames \ntrain_file_path = \"/kaggle/input/playground-series-s3e26/train.csv\"\ntrain_df = pd.read_csv(train_file_path) \ntest_file_path = \"/kaggle/input/playground-series-s3e26/test.csv\"\ntest_df = pd.read_csv(test_file_path)\n\n# Remove the id column since it is not useful for prediction and might confuse the model in training\ntest_id_df = test_df['id'].astype(int)\ntrain_df = train_df.drop('id', axis=1)\ntest_df = test_df.drop('id', axis=1)\n\n# Add data from the original dataset to our training set \norig_df = pd.read_csv(\"/kaggle/input/cirrhosis-patient-survival-prediction/cirrhosis.csv\")[train_df.columns]\n\n# Merge supplementary data\ntrain_df = pd.concat(objs=[train_df, orig_df]).reset_index(drop=True)\n\n# Use the head method to visually see that the dataset has been loaded\ntrain_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T22:31:55.886275Z","iopub.execute_input":"2024-01-01T22:31:55.886664Z","iopub.status.idle":"2024-01-01T22:31:55.948424Z","shell.execute_reply.started":"2024-01-01T22:31:55.886624Z","shell.execute_reply":"2024-01-01T22:31:55.947155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Combine the synthetic and original dataset to obtain more training data \ntrain_df = pd.concat([train_df, orig_df]).reset_index(drop=True)\n\nprint(train_df.shape)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T22:31:55.950085Z","iopub.execute_input":"2024-01-01T22:31:55.950593Z","iopub.status.idle":"2024-01-01T22:31:55.961567Z","shell.execute_reply.started":"2024-01-01T22:31:55.950562Z","shell.execute_reply":"2024-01-01T22:31:55.960185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use info to see the number of categorical/text columns (any column with Dtype=object is text and will need to be converted to a number since ML models work only with numbers) \n# Here you can also see if there are any missing values by looking at the number of non-null values in each column\ntrain_df.info()\n\n# Make a list of categorical columns for future use (in the feature scaling section)\ncategorical_cols = ['Drug', 'Sex', 'Ascites', 'Hepatomegaly', 'Spiders', 'Edema', 'Status']","metadata":{"execution":{"iopub.status.busy":"2024-01-01T22:31:55.96257Z","iopub.execute_input":"2024-01-01T22:31:55.962973Z","iopub.status.idle":"2024-01-01T22:31:55.983048Z","shell.execute_reply.started":"2024-01-01T22:31:55.962942Z","shell.execute_reply":"2024-01-01T22:31:55.981216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use the describe method to generate statistical information such as standard deviation, mean, and min/max. \ntrain_df.describe()","metadata":{"execution":{"iopub.status.busy":"2024-01-01T22:31:55.985541Z","iopub.execute_input":"2024-01-01T22:31:55.985968Z","iopub.status.idle":"2024-01-01T22:31:56.032574Z","shell.execute_reply.started":"2024-01-01T22:31:55.985906Z","shell.execute_reply":"2024-01-01T22:31:56.031472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset Pre-Processing Task Overview\n\n1. Target Variable Label Encoding - Make sure the training labels are numerical, otherwise we cannot train the model. \n2. Missing Feature Value Handling - Make sure the dataset is not missing any values, if so we cannot train the model.\n3. Feature Scaling - Make sure the features are similar in numerical value, otherwise some ML models will struggle to weigh them appropriately during training.  \n4. Categorical Attribute Handling (Should be done at the end since it may change the number of columns) - Make sure text values have been converted to numbers, otherwise ML algorithms cannot learn from text data.\n5. Train/Test Split - Need to create a validation set for hyperparameter tuning and evaluation, otherwise the model will overfit the hyperparameters to the training dataset and your model will not generalize well to the real-world / Kaggle private test set. ","metadata":{}},{"cell_type":"markdown","source":"## Target Variable Label Encoding\n\nProblem: Since the Label column that we are trying to predict, 'Status', is a text column, we need to convert the column's values into integers so the machine learning model can process it (ML models do not support text input so we must convert the text into a numerical representation). We have three possible categories: Status_C, Status_CL, and Status_D. \n\nPossible Approaches:\n1. Label Encoding - Convert each text category into an integer label. (Ex: 0, 1, 2)\n2. Ordinal Encoding - Convert each text category into an integer label but with a particular order. Used when the categories have some quantitative order that can be taken advantage of like low, medium, high.   (Ex: low -> 0, medium-> 1, high -> 2)\n3. One-Hot Encoding - Convert each text category into a separate column. For example, this is done in the softmax layer of a neural network. \n\nChosen Approach: Label Encoding - There is no obvious ordering in the 'Status' column and ML libraries typically expect a single Label column which rules out one-hot encoding. Therefore we will use label encoding.","metadata":{}},{"cell_type":"code","source":"# Check to see what the initial text categories are\ntrain_df['Status'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-01-01T22:31:56.034472Z","iopub.execute_input":"2024-01-01T22:31:56.03485Z","iopub.status.idle":"2024-01-01T22:31:56.044634Z","shell.execute_reply.started":"2024-01-01T22:31:56.034817Z","shell.execute_reply":"2024-01-01T22:31:56.043466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n# Use sklearn's LabelEncoder class to transform the Status column's values from strings to integers.\nlabel_encoder = LabelEncoder() \ntrain_df['Status'] = label_encoder.fit_transform(train_df['Status'])\n\n# Check that the label encoder transformation was applied correctly and the categories are now numbers\ntrain_df['Status'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-01-01T22:31:56.046104Z","iopub.execute_input":"2024-01-01T22:31:56.046512Z","iopub.status.idle":"2024-01-01T22:31:56.059887Z","shell.execute_reply.started":"2024-01-01T22:31:56.046467Z","shell.execute_reply":"2024-01-01T22:31:56.058129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Missing Feature Value Handling\n\nProblem: We need to check if there are any missing feature values since most machine learning algorithms cannot handle an empty cell with no value. For example, a Logistic Regression model would throw an error in training although some ML algorithms like XGBoost are implemented to train normally by substituting in a value as needed. \n\nSolution: We check for missing feature values by using the isna DataFrame method which returns a boolean DataFrame where each cell is True if the value is missing and False otherwise. We then apply the sum method to find the number of missing values in each column. \n\nIn the synthetic dataset there are no missing values, however, after combining the original and synthetic datasets there are many missing values in various columns. We could try to replace missing values with median/mean values but for simplicity we choose to simply drop columns that are missing values. ","metadata":{}},{"cell_type":"code","source":"# Check for missing feature values\nprint(\"Number of missing feature values by column: \")\nprint(train_df.isna().sum())","metadata":{"execution":{"iopub.status.busy":"2024-01-01T22:31:56.061199Z","iopub.execute_input":"2024-01-01T22:31:56.061539Z","iopub.status.idle":"2024-01-01T22:31:56.074855Z","shell.execute_reply.started":"2024-01-01T22:31:56.061509Z","shell.execute_reply":"2024-01-01T22:31:56.073733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Alternatively we can check the total number of missing feature values in the dataset this way. \nprint(\"Number of missing feature values by column in test data:\", train_df.isna().sum().sum())","metadata":{"execution":{"iopub.status.busy":"2024-01-01T22:31:56.076029Z","iopub.execute_input":"2024-01-01T22:31:56.076546Z","iopub.status.idle":"2024-01-01T22:31:56.08753Z","shell.execute_reply.started":"2024-01-01T22:31:56.076518Z","shell.execute_reply":"2024-01-01T22:31:56.086549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove rows that are missing any feature values, then shuffle the training examples\n\ntrain_df = train_df.dropna()\ntrain_df = train_df.sample(frac = 1).reset_index(drop = True)\n\nprint(train_df.isna().sum())\nprint(train_df.shape)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T22:31:56.088637Z","iopub.execute_input":"2024-01-01T22:31:56.088953Z","iopub.status.idle":"2024-01-01T22:31:56.105197Z","shell.execute_reply.started":"2024-01-01T22:31:56.088926Z","shell.execute_reply":"2024-01-01T22:31:56.104291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Scaling\n\nProblem: Typically since feature column values are combined (often by adding them together) to create the final classification, the ML model will perform better if the features are on the same scale. (Ex: The ML model would struggle to scale values correctly if one column's values was in the billions and another column had values from 1-10)\n\nPossible Feature Scaling Approaches: \n1. Min-Max Scaling (Default)- Scales the data to a fixed range between two values (typically 0 and 1). Most useful for neural networks.\n2. Standardization (Default) - Scales the data so that the mean is 0 and the standard deviation is 1. Most useful for algorithms that assume a normal distribution of data, such as SVMs and logistic regression.\n3. Robust Scaling (Advanced) - Scaling based on median and IQR. Most useful for handling significant outliers.\n4. MaxAbsScaler (Advanced) - Scales each feature based on its maximum absolute value. Useful for sparse data. \n\nChosen Approach: Standardization - Since XGBoost's decision tree classification uses splitting which occurs within a column, different column values do not interact with each other and therefore scaling the features is not necessary. However, since we are using Logistic Regression as our baseline model and features do interact in training we will need to do feature scaling. Since we are doing feature scaling specifically for our logistic regression model we will use the Standardization approach which is a commonly used approach that is effective for many datasets. ","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler \n\n# Here we find the numerical columns that need to be scaled by removing the text columns from the list of total columns in the df.\nnumerical_cols = train_df.columns.difference(categorical_cols)\n\n# Now we use the StandardScaler class from sklearn to transform our numerical columns to the a Standardized scale\nstd_scaler = StandardScaler() \nstd_scaler.fit(train_df[numerical_cols])\n\ntrain_df[numerical_cols] = std_scaler.transform(train_df[numerical_cols])\ntest_df[numerical_cols] = std_scaler.transform(test_df[numerical_cols])\n                      \n# Confirm the transformation was successful by seeing if the mean = 0 and std = 1 for numerical columns\ntest_df.describe()","metadata":{"execution":{"iopub.status.busy":"2024-01-01T22:31:56.107812Z","iopub.execute_input":"2024-01-01T22:31:56.108139Z","iopub.status.idle":"2024-01-01T22:31:56.158559Z","shell.execute_reply.started":"2024-01-01T22:31:56.108114Z","shell.execute_reply":"2024-01-01T22:31:56.157506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Handling Categorical Attributes/Columns \nProblem: ML models cannot handle text data naturally, they can only handle numbers so we need to convert text data into some numerical representation that still contains the relevant information. \n\nPossible Approaches: The main approaches for categorical attribute handling are \n1. Ordinal Encoding - Useful when the categories correspond to an ascending or descending order. \n2. One-Hot Encoding (Default Choice) - For each categorical column, convert it into multiple columns, one for each possible category. This is used when the categories do not have an obvious logical order. \n3. Numerical Feature Replacement (Advanced) - In cases where the number of categories is cery large (hundreds or thousands) one should consider replacing the categorical columns with a numerical column that converts each category into some number. For example, one could convert a country code into the country's population. \n4. Embedding Replacement (Advanced) - Alternatively, one can replace categories with embeddings, which are low dimensional vectors that represent the category. \n\nChosen Approach : One-Hot Encoding - In this case, we use a one-hot encoding since none of the categories seem to have an order to the classes (ruling out ordinal encoding) and the number of categories for each column is low (under 10 for all categorical columns) which rules out needing advanced methods. ","metadata":{}},{"cell_type":"code","source":"# Confirm that the number of categories in the categorical columns is manageable (< 100) since we will be adding a column to the df for each category\nunique_values_per_column = train_df.nunique()\nprint(unique_values_per_column)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T22:31:56.159906Z","iopub.execute_input":"2024-01-01T22:31:56.160164Z","iopub.status.idle":"2024-01-01T22:31:56.17147Z","shell.execute_reply.started":"2024-01-01T22:31:56.160141Z","shell.execute_reply":"2024-01-01T22:31:56.17054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert the categorical columns into one-hot encodings using the get_dummies function\nstatus = train_df['Status']\ntrain_df_dummies = pd.get_dummies(train_df.drop('Status', axis=1))\ntrain_df = pd.concat([train_df_dummies, status], axis=1)\ntest_df = pd.get_dummies(test_df)\n\n# Confirm the transformation was successful\ntest_df.info()","metadata":{"execution":{"iopub.status.busy":"2024-01-01T22:31:56.172598Z","iopub.execute_input":"2024-01-01T22:31:56.173407Z","iopub.status.idle":"2024-01-01T22:31:56.207574Z","shell.execute_reply.started":"2024-01-01T22:31:56.173381Z","shell.execute_reply":"2024-01-01T22:31:56.206345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train/Test Split\n\nProblem: We need to split the dataset into a training dataset and a testing/tuning dataset. If we were not to do this and did both training / tuning / evaluation with the same dataset, the final model would likely be overfitted and would not generalize well to the real-world / the Kaggle competition private test set. \n\nPossible Approaches\n1. K-Fold Cross-Validation - In this approach the dataset is divided into k equal-sized subsets. Then we train the model k times, each time using a different subset as the validation set and the other k-1 subsets as the training set. Finally we use the average score among all k models as the final score. This approach is commonly used since it essentially utilizes  more of the training data for validation (90% vs. 80%). Typical values range are k=5 or k=10. \n2. Train/Validation/Test Split - In this approach we split the dataset into three distinct sets: a training set for training the model, a validation set for tuning the hyperparameters and a test set for evaluating the final model. By separating the test set and validation set we reduce / avoid overfitting to the test set. A typical split varies but could be 80/10/10 or 70/15/15.\n\nChosen Approach : Initially we used the train/validation/test approach since it is simpler to implement for manual hyperparameter tuning. However later we noticed that the log-loss scores fluctuated a lot depending on what training examples were placed in the validation set, so to reduce this effect we switched to K-Fold Cross Validation for hyperparameter tuning with k=5. ","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\n\n# Set parameters for k-folds cross-validation\nkfold = StratifiedKFold(n_splits=10, shuffle=True)\n\nfrom sklearn.model_selection import train_test_split\n\n# Split the training set into a training and validation set \nX = train_df.drop(\"Status\", axis=1)\ny = train_df[\"Status\"]\n\nX_train, X_val, y_train, y_val = train_test_split(X, y,  test_size = 0.2, random_state=8)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-01T22:31:56.209192Z","iopub.execute_input":"2024-01-01T22:31:56.209469Z","iopub.status.idle":"2024-01-01T22:31:56.332517Z","shell.execute_reply.started":"2024-01-01T22:31:56.209445Z","shell.execute_reply":"2024-01-01T22:31:56.331445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Logistic Regression Baseline Model\nHyperparameter Tuning: \nHere we manually tuned the most important parameters for Logistic Regression (C, max_iter, class_weight, tol, penalty, solver) but were unable to significantly increase performance (still around 0.526)\n\nConclusion: We set the baseline of model performance at 0.526 log-loss since Logistic Regression is the simplest model we used on the dataset. Since the performance was so weak, we decided to not include Logistic Regression in the final ensemble of models for competition submission.  ","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression \nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import cross_val_score\n\n# Train the model using sci-kit learn's Logistic Regression model \nmodel = LogisticRegression(C=1, max_iter = 1000)\n\n# Use neg_log_loss as the scoring parameter\nscores = cross_val_score(model, X, y, cv=5, scoring='neg_log_loss')\n\nprint(\"Log loss scores for each fold:\", scores*-1)\nprint(\"Average log loss:\", scores.mean()*-1)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-01T22:31:56.333964Z","iopub.execute_input":"2024-01-01T22:31:56.334531Z","iopub.status.idle":"2024-01-01T22:31:57.611529Z","shell.execute_reply.started":"2024-01-01T22:31:56.334496Z","shell.execute_reply":"2024-01-01T22:31:57.610682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random Forest Model \nHyperparameter Tuning:\nWe manually tuned hyperparameters starting from the most important hyperparameters (n_estimators) and working through the rest (max_depth, max_features, min_samples_split, min_impurity_decrease, min_samples_leaf, min_weight_fraction_leaf). We only found significant improvements in performance by tuning the n_estimator hyperparameter which improved performance from 0.511 -> 0.507.\n\nConclusion: The overall conclusion is that the Random Forest model performed significantly better than the baseline Logistic Regression model with a log-loss of 0.5 compared to 0.53. However, this model still performed significantly worse than the best XGBoost and LGBM models so we decided to exclude the Random Forest from the final ensemble for competition submission. \n","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\n# Train the model using sci-kit learn's Random Forest model \nrf_classifier = RandomForestClassifier()\n\n# Use neg_log_loss as the scoring parameter\nscores = cross_val_score(rf_classifier, X, y, cv=5, scoring='neg_log_loss')\n\nprint(\"Log loss scores for each fold:\", scores*-1)\nprint(\"Average log loss:\", scores.mean()*-1)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T22:31:57.612853Z","iopub.execute_input":"2024-01-01T22:31:57.613382Z","iopub.status.idle":"2024-01-01T22:32:04.770666Z","shell.execute_reply.started":"2024-01-01T22:31:57.613352Z","shell.execute_reply":"2024-01-01T22:32:04.769697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XGBoost Model\nApproach: Initially we tuned the model hyperparameters manually as before which is an approach I've seen suggested in some ML books such as Corey Wade's XGBoost book. However, we were not able to tune the model beyond ~0.43 score. We then looked at what range of hyperparameter values worked in general in other notebooks and tuned within this range which obtained better overall results. \n\nConclusion: After comparing to other XGboost notebooks it was clear in order to tune the model hyperparameters manually one needs a great deal of experience. For example, in the method I have been using where you tune hyperparameters one at a time by simply changing the numbers in the XGBClassifier(n_estimators=700) constructor call, n_estimators=700 performs very poorly and therefore is discarded early in the process. However, it turns out n_estimators=700 performs very well *if* the learning_rate is also tuned at the same time. Instead using more automated approaches with GridSearchCV are necessary to cover a reasonable amount of the hyperparameter search space and to get optimal results. ","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBClassifier \n\n# Train the model using xgboost's XGBClassifier model \nxgb_model = XGBClassifier(n_estimators=750, max_depth=5, learning_rate=0.032, colsample_bytree=0.225, min_child_weight=17, subsample=0.7) \n\n# Use neg_log_loss as the scoring parameter\nscores = cross_val_score(xgb_model, X, y, cv=5, scoring='neg_log_loss')\n\nprint(\"Log loss scores for each fold:\", scores*-1)\nprint(\"Average log loss:\", scores.mean()*-1)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T23:55:24.730196Z","iopub.execute_input":"2024-01-01T23:55:24.730559Z","iopub.status.idle":"2024-01-01T23:55:49.330452Z","shell.execute_reply.started":"2024-01-01T23:55:24.730531Z","shell.execute_reply":"2024-01-01T23:55:49.3287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LGBM Model\nApproach  / Conclusion: The approach and conclusions are similar to those found in the XGBoost section. ","metadata":{}},{"cell_type":"code","source":"import lightgbm as lgb\n\n# Define your parameters\nlgb_params = {\n    'max_depth': 15,\n    'min_child_samples': 13,\n    'learning_rate': 0.05285597081335651,\n    'n_estimators': 294,\n    'min_child_weight': 5,\n    'colsample_bytree': 0.10012816493265511,\n    'reg_alpha': 0.8767668608061822,\n    'reg_lambda': 0.8705834466355764\n}\n\n# Create the LGBMClassifier with the specified parameters\nlgbm_model = lgb.LGBMClassifier(**lgb_params)\n\n# Use neg_log_loss as the scoring parameter\nscores = cross_val_score(lgbm_model, X, y, cv=5, scoring='neg_log_loss')\n\nprint(\"Log loss scores for each fold:\", scores*-1)\nprint(\"Average log loss:\", scores.mean()*-1)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T22:32:15.489197Z","iopub.execute_input":"2024-01-01T22:32:15.48945Z","iopub.status.idle":"2024-01-01T22:33:04.261231Z","shell.execute_reply.started":"2024-01-01T22:32:15.48942Z","shell.execute_reply":"2024-01-01T22:33:04.259342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ensemble Model\n\nApproach: Since our current Logistic Regression and Random Forest models are significantly weaker than the XGBoost and LGBM models, we will only use the tuned XGBoost / LGBM models in the Ensemble. Further work will involve trying to tune the other models further so they can be included in the Ensemble. \n\nTuned Ensemble Model Score (Synthetic Data): 0.431\nTuned Ensemble Model Score (Combined Data) : 0.40925","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\n\n# Train an Ensemble model using a combination of the XGBoost and LGBM Classifiers, voting='soft' is used since we are predicting probabilities, not the actual classes\nensemble_model = VotingClassifier(\n    estimators=[\n        ('lgb', lgbm_model),\n        ('xgb', xgb_model),\n    ],\n    voting='soft'\n)\n\n# Use neg_log_loss as the scoring parameter\nscores = cross_val_score(ensemble_model, X, y, cv=5, scoring='neg_log_loss')\n\nprint(\"Log loss scores for each fold:\", scores*-1)\nprint(\"Average log loss:\", scores.mean()*-1)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T23:02:42.970232Z","iopub.execute_input":"2024-01-01T23:02:42.970588Z","iopub.status.idle":"2024-01-01T23:03:52.226928Z","shell.execute_reply.started":"2024-01-01T23:02:42.970557Z","shell.execute_reply":"2024-01-01T23:03:52.225829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the final Ensemble model for competition submission using all the data, including data from the validation set \nensemble_model_final = VotingClassifier(\n    estimators=[\n        ('lgb', lgbm_model),\n        ('xgb', xgb_model),\n    ],\n    voting='soft'\n)\n\nensemble_model_final.fit(X, y)\n\n# Make predictions using the ensemble\ny_pred_ensemble_final = ensemble_model_final.predict_proba(test_df)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T22:34:09.76314Z","iopub.execute_input":"2024-01-01T22:34:09.763489Z","iopub.status.idle":"2024-01-01T22:34:24.747512Z","shell.execute_reply.started":"2024-01-01T22:34:09.763458Z","shell.execute_reply":"2024-01-01T22:34:24.745985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Kaggle Submission Processing\n1. Create a submission dataframe from the model's predictions \n2. Concatenate the data id column values to adhere to submission formatting requirements\n3. Convert the submission dataframe into a csv file for submission \n4. Now in order to submit to Kaggle, save the notebook and navigate to the Submissions page for this competition and click 'Submit Prediction' in the top-right corner -> Notebook -> Submit. ","metadata":{}},{"cell_type":"code","source":"# Modify the probability predictions into the submission format \nsubmission_df = pd.DataFrame(y_pred_ensemble_final, columns=['Status_C', 'Status_CL', 'Status_D'])\nfinal_submission_df = pd.concat([test_id_df, submission_df], axis=1)\nfinal_submission_df.head(10)\n\n# Create a submission.csv file that Kaggle will automatically evaluate for submission\nfinal_submission_df.to_csv('submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T22:34:24.748478Z","iopub.execute_input":"2024-01-01T22:34:24.74969Z","iopub.status.idle":"2024-01-01T22:34:24.782544Z","shell.execute_reply.started":"2024-01-01T22:34:24.749636Z","shell.execute_reply":"2024-01-01T22:34:24.780425Z"},"trusted":true},"execution_count":null,"outputs":[]}]}